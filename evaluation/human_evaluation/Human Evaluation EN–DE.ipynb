{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import jsonlines\n",
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "language_pair = \"en-de\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Define data paths\n",
    "annotations_path = Path(\".\") / \"annotations\"\n",
    "annotator1_path = annotations_path / f\"{language_pair}.annotator1.jsonl\"\n",
    "annotator2_path = annotations_path / f\"{language_pair}.annotator2.jsonl\"\n",
    "\n",
    "predictions_path = Path(\".\") / \"predictions\"\n",
    "dev_path = predictions_path / f\"{language_pair}.dev.jsonl\"\n",
    "test_path = predictions_path / f\"{language_pair}.test.jsonl\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotations 1 total: 767\n",
      "Annotations 1 OT: 379\n",
      "Annotations 1 UT: 388\n",
      "Annotations 2 total: 760\n",
      "Annotations 2 OT: 366\n",
      "Annotations 2 UT: 394\n"
     ]
    }
   ],
   "source": [
    "# Load annotations\n",
    "with jsonlines.open(annotator1_path) as f:\n",
    "  annotations1 = {(sample[\"seg_id\"], sample[\"system\"]): sample for sample in f if len(sample[\"label\"]) == 2}\n",
    "with jsonlines.open(annotator2_path) as f:\n",
    "  annotations2 = {(sample[\"seg_id\"], sample[\"system\"]): sample for sample in f if len(sample[\"label\"]) == 2}\n",
    "print(\"Annotations 1 total:\", len(annotations1))\n",
    "print(\"Annotations 1 OT:\", len([sample for sample in annotations1.values() if sample[\"coverage_error_type\"] == \"overtranslation\"]))\n",
    "print(\"Annotations 1 UT:\", len([sample for sample in annotations1.values() if sample[\"coverage_error_type\"] == \"undertranslation\"]))\n",
    "print(\"Annotations 2 total:\", len(annotations2))\n",
    "print(\"Annotations 2 OT:\", len([sample for sample in annotations2.values() if sample[\"coverage_error_type\"] == \"overtranslation\"]))\n",
    "print(\"Annotations 2 UT:\", len([sample for sample in annotations2.values() if sample[\"coverage_error_type\"] == \"undertranslation\"]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotations 1 total: 719\n",
      "Annotations 1 OT: 374\n",
      "Annotations 1 UT: 345\n",
      "Annotations 2 total: 706\n",
      "Annotations 2 OT: 352\n",
      "Annotations 2 UT: 354\n"
     ]
    }
   ],
   "source": [
    "# Filter annotations by whether they apply to the final version of the algorithm\n",
    "prediction_keys = set()\n",
    "with jsonlines.open(dev_path) as f:\n",
    "  prediction_keys |= {(sample[\"seg_id\"], sample[\"system\"]) for sample in f}\n",
    "with jsonlines.open(test_path) as f:\n",
    "  prediction_keys |= {(sample[\"seg_id\"], sample[\"system\"]) for sample in f}\n",
    "\n",
    "annotations1 = {key: value for key, value in annotations1.items() if key in prediction_keys and \"source-error\" not in value[\"label\"]}\n",
    "annotations2 = {key: value for key, value in annotations2.items() if key in prediction_keys and \"source-error\" not in value[\"label\"]}\n",
    "print(\"Annotations 1 total:\", len(annotations1))\n",
    "print(\"Annotations 1 OT:\", len([sample for sample in annotations1.values() if sample[\"coverage_error_type\"] == \"overtranslation\"]))\n",
    "print(\"Annotations 1 UT:\", len([sample for sample in annotations1.values() if sample[\"coverage_error_type\"] == \"undertranslation\"]))\n",
    "print(\"Annotations 2 total:\", len(annotations2))\n",
    "print(\"Annotations 2 OT:\", len([sample for sample in annotations2.values() if sample[\"coverage_error_type\"] == \"overtranslation\"]))\n",
    "print(\"Annotations 2 UT:\", len([sample for sample in annotations2.values() if sample[\"coverage_error_type\"] == \"undertranslation\"]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of overlapping samples:  474\n"
     ]
    }
   ],
   "source": [
    "# Extract overlapping samples for inter-annotator agreement\n",
    "overlap_samples1 = {key: sample for key, sample in annotations1.items() if key in annotations2}\n",
    "overlap_samples2 = {key: sample for key, sample in annotations2.items() if key in annotations1}\n",
    "assert len(overlap_samples1) == len(overlap_samples2)\n",
    "print(\"Number of overlapping samples: \", len(overlap_samples1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5352599455842342\n"
     ]
    }
   ],
   "source": [
    "# Question 1 inter-annotator agreement\n",
    "question1_labels1 = [\"bad-translation\" in overlap_samples1[key][\"label\"] for key in sorted(overlap_samples1)]\n",
    "question1_labels2 = [\"bad-translation\" in overlap_samples2[key][\"label\"] for key in sorted(overlap_samples1)]\n",
    "question1_kappa = cohen_kappa_score(question1_labels1, question1_labels2)\n",
    "print(question1_kappa)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3187518043857557\n"
     ]
    }
   ],
   "source": [
    "# Question 1+2 inter-annotator agreement\n",
    "question2_labels1 = [str(sorted(overlap_samples1[key][\"label\"])) for key in sorted(overlap_samples1)]\n",
    "question2_labels2 = [str(sorted(overlap_samples2[key][\"label\"])) for key in sorted(overlap_samples1)]\n",
    "question2_kappa = cohen_kappa_score(question2_labels1, question2_labels2)\n",
    "print(question2_kappa)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overtranslation\tbad-translation\t49\n",
      "overtranslation\tgood-translation\t617\n",
      "undertranslation\tbad-translation\t241\n",
      "undertranslation\tgood-translation\t371\n",
      "overtranslation\tbad-translation+OT-supported-information\t10\n",
      "overtranslation\tbad-translation+OT-unsupported-information\t5\n",
      "overtranslation\tbad-translation+UT-important-information\t0\n",
      "overtranslation\tbad-translation+UT-redundant-information\t0\n",
      "overtranslation\tbad-translation+other-error-accuracy\t28\n",
      "overtranslation\tbad-translation+other-error-fluency\t6\n",
      "overtranslation\tgood-translation+OT-fluency\t113\n",
      "overtranslation\tgood-translation+OT-supported-information\t19\n",
      "overtranslation\tgood-translation+UT-fluency\t0\n",
      "overtranslation\tgood-translation+UT-redundant-information\t0\n",
      "overtranslation\tgood-translation+syntactic-difference\t434\n",
      "overtranslation\tgood-translation+unclear\t51\n",
      "undertranslation\tbad-translation+OT-supported-information\t0\n",
      "undertranslation\tbad-translation+OT-unsupported-information\t0\n",
      "undertranslation\tbad-translation+UT-important-information\t114\n",
      "undertranslation\tbad-translation+UT-redundant-information\t108\n",
      "undertranslation\tbad-translation+other-error-accuracy\t16\n",
      "undertranslation\tbad-translation+other-error-fluency\t3\n",
      "undertranslation\tgood-translation+OT-fluency\t0\n",
      "undertranslation\tgood-translation+OT-supported-information\t0\n",
      "undertranslation\tgood-translation+UT-fluency\t72\n",
      "undertranslation\tgood-translation+UT-redundant-information\t25\n",
      "undertranslation\tgood-translation+syntactic-difference\t251\n",
      "undertranslation\tgood-translation+unclear\t23\n"
     ]
    }
   ],
   "source": [
    "question1_counter = Counter()\n",
    "question2_counter = Counter()\n",
    "span_counters = defaultdict(Counter)\n",
    "for sample in itertools.chain(annotations1.values(), annotations2.values()):\n",
    "  if sample[\"split\"] != \"test\":\n",
    "    continue\n",
    "  for label in [\"good-translation\", \"bad-translation\"]:\n",
    "    question1_counter[sample[\"coverage_error_type\"] + \"_\" + label] += label in sample[\"label\"]\n",
    "  for label_pair in [\n",
    "    (\"good-translation\", \"OT-supported-information\"),\n",
    "    (\"good-translation\", \"OT-fluency\"),\n",
    "    (\"good-translation\", \"UT-redundant-information\"),\n",
    "    (\"good-translation\", \"UT-fluency\"),\n",
    "    (\"good-translation\", \"syntactic-difference\"),\n",
    "    (\"good-translation\", \"unclear\"),\n",
    "    (\"bad-translation\", \"OT-unsupported-information\"),\n",
    "    (\"bad-translation\", \"OT-supported-information\"),\n",
    "    (\"bad-translation\", \"UT-important-information\"),\n",
    "    (\"bad-translation\", \"UT-redundant-information\"),\n",
    "    (\"bad-translation\", \"other-error-accuracy\"),\n",
    "    (\"bad-translation\", \"other-error-fluency\"),\n",
    "  ]:\n",
    "    label = sample[\"coverage_error_type\"] + \"_\" + \"+\".join(label_pair)\n",
    "    question2_counter[label] += set(label_pair) == set(sample[\"label\"])\n",
    "    for span in itertools.chain(sample[\"predicted_overtranslation_words\"].split(\" | \"), sample[\"predicted_undertranslation_words\"].split(\" | \")):\n",
    "      if span.strip() and set(label_pair) == set(sample[\"label\"]):\n",
    "        span_counters[label][span] += 1\n",
    "for key in sorted(list(question1_counter)):\n",
    "  print(key.replace(\"_\", \"\\t\") + \"\\t\" + str(question1_counter[key]))\n",
    "for key in sorted(list(question2_counter)):\n",
    "  print(key.replace(\"_\", \"\\t\") + \"\\t\" + str(question2_counter[key]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}